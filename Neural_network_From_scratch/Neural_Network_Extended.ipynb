{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNe6IxupWQ35"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import List, Tuple, Optional\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "class NeuralNetwork:\n",
        "    \"\"\"\n",
        "    A flexible implementation of a feedforward neural network using NumPy.\n",
        "    Supports multiple hidden layers and mini-batch gradient descent.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size: int, hidden_layers_sizes: List[int], output_size: int):\n",
        "        \"\"\"\n",
        "        Initialize the neural network with the specified architecture.\n",
        "\n",
        "        Args:\n",
        "            input_size: Number of input features\n",
        "            hidden_layers_sizes: List containing the size of each hidden layer\n",
        "            output_size: Number of output neurons\n",
        "        \"\"\"\n",
        "        self.input_size = input_size\n",
        "        self.hidden_layers_sizes = hidden_layers_sizes\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Initialize network architecture\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        # Input layer to first hidden layer\n",
        "        layer_sizes = [input_size] + hidden_layers_sizes + [output_size]\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            # Xavier/Glorot initialization\n",
        "            limit = np.sqrt(6 / (layer_sizes[i] + layer_sizes[i + 1]))\n",
        "            self.weights.append(\n",
        "                np.random.uniform(-limit, limit, (layer_sizes[i], layer_sizes[i + 1]))\n",
        "            )\n",
        "            self.biases.append(np.zeros((1, layer_sizes[i + 1])))\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute the sigmoid activation function with numerical stability.\n",
        "\n",
        "        Args:\n",
        "            x: Input array\n",
        "\n",
        "        Returns:\n",
        "            Sigmoid activation output\n",
        "        \"\"\"\n",
        "        x_clipped = np.clip(x, -500, 500)  # Prevent overflow\n",
        "        return 1 / (1 + np.exp(-x_clipped))\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute the derivative of the sigmoid function.\n",
        "\n",
        "        Args:\n",
        "            x: Sigmoid activation output\n",
        "\n",
        "        Returns:\n",
        "            Derivative of sigmoid function\n",
        "        \"\"\"\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Perform forward propagation through the network.\n",
        "\n",
        "        Args:\n",
        "            X: Input features\n",
        "\n",
        "        Returns:\n",
        "            Network predictions\n",
        "        \"\"\"\n",
        "        # Input validation and reshaping\n",
        "        X = np.array(X)\n",
        "        if X.ndim == 1:\n",
        "            X = X.reshape(1, -1)\n",
        "\n",
        "        self.activations = [X]\n",
        "\n",
        "        # Forward propagation through each layer\n",
        "        for i in range(len(self.weights)):\n",
        "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
        "            activation = self.sigmoid(z)\n",
        "            self.activations.append(activation)\n",
        "\n",
        "        return self.activations[-1]\n",
        "\n",
        "    def backward(self, X: np.ndarray, y: np.ndarray, learning_rate: float) -> None:\n",
        "        \"\"\"\n",
        "        Perform backpropagation to update network weights.\n",
        "\n",
        "        Args:\n",
        "            X: Input features\n",
        "            y: Target values\n",
        "            learning_rate: Learning rate for gradient descent\n",
        "        \"\"\"\n",
        "        # Ensure proper array formatting\n",
        "        y = np.array(y)\n",
        "        if y.ndim == 1:\n",
        "            y = y.reshape(-1, 1)\n",
        "\n",
        "        # Calculate initial error\n",
        "        error = y - self.activations[-1]\n",
        "        delta = error * self.sigmoid_derivative(self.activations[-1])\n",
        "\n",
        "        # Store gradients\n",
        "        weight_gradients = []\n",
        "        bias_gradients = []\n",
        "\n",
        "        # Backpropagate through layers\n",
        "        for i in range(len(self.weights) - 1, -1, -1):\n",
        "            # Calculate gradients\n",
        "            weight_grad = np.dot(self.activations[i].T, delta)\n",
        "            bias_grad = np.sum(delta, axis=0, keepdims=True)\n",
        "\n",
        "            # Gradient clipping\n",
        "            weight_grad = np.clip(weight_grad, -1, 1)\n",
        "            bias_grad = np.clip(bias_grad, -1, 1)\n",
        "\n",
        "            weight_gradients.insert(0, weight_grad)\n",
        "            bias_gradients.insert(0, bias_grad)\n",
        "\n",
        "            if i > 0:\n",
        "                delta = np.dot(delta, self.weights[i].T) * self.sigmoid_derivative(self.activations[i])\n",
        "\n",
        "        # Update weights and biases\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] += learning_rate * weight_gradients[i]\n",
        "            self.biases[i] += learning_rate * bias_gradients[i]\n",
        "\n",
        "    def train(self,\n",
        "             X: np.ndarray,\n",
        "             y: np.ndarray,\n",
        "             epochs: int,\n",
        "             learning_rate: float,\n",
        "             batch_size: Optional[int] = None,\n",
        "             verbose: bool = True) -> List[float]:\n",
        "        \"\"\"\n",
        "        Train the neural network using mini-batch gradient descent.\n",
        "\n",
        "        Args:\n",
        "            X: Input features\n",
        "            y: Target values\n",
        "            epochs: Number of training iterations\n",
        "            learning_rate: Learning rate for gradient descent\n",
        "            batch_size: Size of mini-batches (optional)\n",
        "            verbose: Whether to print training progress\n",
        "\n",
        "        Returns:\n",
        "            List of loss values during training\n",
        "        \"\"\"\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "\n",
        "        if batch_size is None:\n",
        "            batch_size = len(X)\n",
        "\n",
        "        losses = []\n",
        "\n",
        "        try:\n",
        "            for epoch in range(epochs):\n",
        "                # Shuffle data\n",
        "                indices = np.random.permutation(len(X))\n",
        "                X_shuffled = X[indices]\n",
        "                y_shuffled = y[indices]\n",
        "\n",
        "                # Mini-batch training\n",
        "                for i in range(0, len(X), batch_size):\n",
        "                    batch_X = X_shuffled[i:i + batch_size]\n",
        "                    batch_y = y_shuffled[i:i + batch_size]\n",
        "\n",
        "                    self.forward(batch_X)\n",
        "                    self.backward(batch_X, batch_y, learning_rate)\n",
        "\n",
        "                # Calculate and store loss\n",
        "                if verbose and epoch % 100 == 0:\n",
        "                    predictions = self.forward(X)\n",
        "                    loss = np.mean(np.square(y - predictions))\n",
        "                    accuracy = np.mean((predictions > 0.5) == y)\n",
        "                    losses.append(loss)\n",
        "                    logging.info(f\"Epoch {epoch}/{epochs} - Loss: {loss:.6f} - Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Training error: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "        return losses\n",
        "\n",
        "    def evaluate(self, X: np.ndarray, y: np.ndarray) -> Tuple[float, float]:\n",
        "        \"\"\"\n",
        "        Evaluate the network's performance on test data.\n",
        "\n",
        "        Args:\n",
        "            X: Test features\n",
        "            y: Test targets\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (loss, accuracy)\n",
        "        \"\"\"\n",
        "        predictions = self.forward(X)\n",
        "        loss = np.mean(np.square(y - predictions))\n",
        "        accuracy = np.mean((predictions > 0.5) == y)\n",
        "        return loss, accuracy\n",
        "\n",
        "def main():\n",
        "    \"\"\"Example usage with the XOR problem.\"\"\"\n",
        "    # XOR problem dataset\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "    y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "    try:\n",
        "        # Create network\n",
        "        nn = NeuralNetwork(input_size=2, hidden_layers_sizes=[4], output_size=1)\n",
        "\n",
        "        # Train network\n",
        "        losses = nn.train(X, y, epochs=5000, learning_rate=0.1, batch_size=4)\n",
        "\n",
        "        # Evaluate and print results\n",
        "        loss, accuracy = nn.evaluate(X, y)\n",
        "        print(\"\\nFinal Results:\")\n",
        "        print(f\"Loss: {loss:.6f}\")\n",
        "        print(f\"Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "        # Show predictions\n",
        "        predictions = nn.forward(X)\n",
        "        print(\"\\nPredictions:\")\n",
        "        for i in range(len(X)):\n",
        "            print(f\"Input: {X[i]} -> Predicted: {predictions[i][0]:.4f} (Expected: {y[i][0]})\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in main: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QkZkQ2oLWWoq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}